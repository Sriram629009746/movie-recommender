{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\", preload_content=False)\n",
        "\n",
        "with open(\"movie.zip\", 'wb') as out:\n",
        "  while True:\n",
        "    data = req.read(4096)\n",
        "    if not data:\n",
        "      break\n",
        "    out.write(data)\n",
        "req.release_conn()\n",
        "\n",
        "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
        "for fileM in zFile.namelist():\n",
        "  zFile.extract(fileM)"
      ],
      "metadata": {
        "id": "fcZSAoCAFDyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754cb2be-9172-4337-9159-fadb033aae64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ml-latest-small/"
      ],
      "metadata": {
        "id": "YgIIGQ3lIgRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5964e0-865f-4046-e58c-51a96d4b46ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "links.csv  movies.csv  ratings.csv  README.txt\ttags.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Implement Apriori Algorithm\n",
        "In this section, we implement the Apriori algorithm with pruning"
      ],
      "metadata": {
        "id": "QfwWA4d2Ne6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def get_support(itemset, freq_itemset_frequencies):\n",
        "  return frequencies_of_freq_itemsets[itemset]\n",
        "\n",
        "def get_candidate_itemsets(itemsets, length):\n",
        "  candidates = set()\n",
        "  for itemset in itemsets:\n",
        "    for other_itemset in itemsets:\n",
        "      if(len(itemset.union(other_itemset)) == length):\n",
        "        candidates.add(itemset.union(other_itemset))\n",
        "\n",
        "  return candidates\n",
        "\n",
        "def get_freq_itemsets(itemsets, txs_list,  minsup, freq_itemset_frequencies):\n",
        "\n",
        "  freq_itemset_set = set()\n",
        "  for itemset in itemsets:\n",
        "    #print(itemset)\n",
        "    sup = 0\n",
        "    for tx in txs_list:\n",
        "      if(itemset.issubset(tx) == True):\n",
        "        sup +=1\n",
        "    #sup =  get_support(itemset, txs_list, minsup)\n",
        "    if(sup >= minsup):\n",
        "      freq_itemset_set.add(itemset)\n",
        "      freq_itemset_frequencies[itemset] = sup # for confidence calculation\n",
        "\n",
        "  return freq_itemset_set\n"
      ],
      "metadata": {
        "id": "48ITOQG09fT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "\n",
        "#keep reviews with ratings >=3 only\n",
        "ratings_new = allRatings[allRatings['rating']>=3]\n",
        "\n",
        "#initial itemset is the set of singleton movie ids\n",
        "ratings_new = ratings_new.groupby('userId')\n",
        "initial_candidate_list = set() #set having individual movies as sets\n",
        "txs_list = [] #database of transactions\n",
        "for userId, data in ratings_new:\n",
        "  #each transaction(movies reviewed by a given user) is a frozen set\n",
        "  txs_list.append(frozenset(data['movieId']))\n",
        "  for movie in data['movieId']:\n",
        "    # initial_candidate_list is the set of all unique movies reviewed by the users\n",
        "    initial_candidate_list.add(frozenset([movie]))\n",
        "\n",
        "print(\"Size of candidate itemsets for k = 1: {}\".format(len(initial_candidate_list)))\n",
        "\n",
        "#keep track of the support of all the frequent itemsets which form the keys of this dictionary.\n",
        "#this will be used during the calculation of the confidence values\n",
        "frequencies_of_freq_itemsets = defaultdict(int)\n",
        "\n",
        "minsup_val = 150\n",
        "singleton_freq_itemset = get_freq_itemsets(initial_candidate_list, txs_list, minsup_val, frequencies_of_freq_itemsets)\n",
        "print(\"Size of filtered itemsets for k = 1: {}\".format(len(singleton_freq_itemset)))\n",
        "\n",
        "final_itemsets = list() #list of sets where each set contains itemsets indexed by the length of itemsets\n",
        "final_itemsets.append(set()) # for k=0 , dummy null set\n",
        "final_itemsets.append(singleton_freq_itemset)\n",
        "\n",
        "k = 2\n",
        "prev = singleton_freq_itemset\n",
        "\n",
        "while(len(prev) > 1):\n",
        "  #create new extended itemsets\n",
        "  candidates = get_candidate_itemsets(prev, k)\n",
        "  prev = candidates\n",
        "  print(\"Size of candidate itemsets for k = {}: {}\".format(k, len(candidates)))\n",
        "  if(len(candidates) > 0):\n",
        "    result = get_freq_itemsets(candidates, txs_list, minsup_val, frequencies_of_freq_itemsets)\n",
        "    if(len(result)>0):\n",
        "      final_itemsets.append(result)\n",
        "    prev = result\n",
        "    print(\"Size of filtered itemsets for k = {}: {}\".format(k, len(result)))\n",
        "  k=k+1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31pW1XPKJ7YJ",
        "outputId": "023dc1e6-dd6c-43a3-a87d-ee3efb072cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of candidate itemsets for k = 1: 8452\n",
            "Size of filtered itemsets for k = 1: 37\n",
            "Size of candidate itemsets for k = 2: 666\n",
            "Size of filtered itemsets for k = 2: 30\n",
            "Size of candidate itemsets for k = 3: 87\n",
            "Size of filtered itemsets for k = 3: 2\n",
            "Size of candidate itemsets for k = 4: 1\n",
            "Size of filtered itemsets for k = 4: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Print Your Association Rules\n",
        "\n",
        "Next lets print the final association rules in the following format:\n",
        "\n",
        "**movie_name_1, movie_name_2, ... -->\n",
        "movie_name_k**\n",
        "\n",
        "where the movie names can be fetched by joining the movieId with the file `movies.csv`. For example, one rule that you might find is:\n",
        "\n",
        "**Matrix, The (1999),  Star Wars: Episode V - The Empire Strikes Back (1980),  Star Wars: Episode IV - A New Hope (1977),  ->\n",
        "Star Wars: Episode VI - Return of the Jedi (1983)**"
      ],
      "metadata": {
        "id": "Ea6GbdOOBZOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "import itertools\n",
        "\n",
        "minconf = 0.8\n",
        "\n",
        "movie_info = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
        "\n",
        "def genAssociateRules(itemset, minconf, frequencies):\n",
        "  '''\n",
        "  returns the association rules for a given itemset given\n",
        "  the itemset, min confidence value and the frequencies(support) of all itemsets\n",
        "  '''\n",
        "  rules =[]\n",
        "  rulecount = 0\n",
        "\n",
        "  for length in range(1,len(itemset)+1):\n",
        "    subsets_list = list(itertools.combinations(itemset, length))\n",
        "    for subset in subsets_list:\n",
        "      #association rule : antecedent => consequent\n",
        "      antecedent = frozenset(itemset.difference(subset))\n",
        "      if(len(antecedent)>0 and len(antecedent)<len(itemset)):\n",
        "        num = frequencies[itemset]\n",
        "        den = frequencies[antecedent]\n",
        "        conf = float(num)/den\n",
        "        if(conf >= minconf):\n",
        "          rulecount+=1\n",
        "          #get the corresponding movie names from the ids\n",
        "          antecedent_movies = []\n",
        "          consequent_movies = []\n",
        "          for id in antecedent:\n",
        "            antecedent_movies.append(movie_info[movie_info['movieId'] == id]['title'].values[0])\n",
        "          for id in subset:\n",
        "            consequent_movies.append(movie_info[movie_info['movieId'] == id]['title'].values[0])\n",
        "          rules.append(tuple([antecedent_movies, consequent_movies]))\n",
        "\n",
        "  return rules"
      ],
      "metadata": {
        "id": "rSbhiIkw9kj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "\n",
        "#keep reviews with ratings >=3 only\n",
        "ratings_new = allRatings[allRatings['rating']>=3]\n",
        "\n",
        "#initial itemset is the set of singleton movie ids\n",
        "ratings_new = ratings_new.groupby('userId')\n",
        "initial_candidate_list = set() #set having individual movies as sets\n",
        "txs_list = [] #database of transactions\n",
        "for userId, data in ratings_new:\n",
        "  #each transaction(movies reviewed by a given user) is a frozen set\n",
        "  txs_list.append(frozenset(data['movieId']))"
      ],
      "metadata": {
        "id": "76X8eNRSd2yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "association_rules = []\n",
        "for itemsets_of_len_k in final_itemsets:\n",
        "  for itemset in itemsets_of_len_k:\n",
        "    rules_list = genAssociateRules(itemset, minconf, frequencies_of_freq_itemsets)\n",
        "    if(len(rules_list)>0):\n",
        "      for rule in rules_list:\n",
        "        formatted_rule = ''\n",
        "        for movie in rule[0]:\n",
        "          formatted_rule = formatted_rule + str(movie) + ','\n",
        "        formatted_rule = formatted_rule.rstrip(',') + ' => '\n",
        "        for movie in rule[1]:\n",
        "          formatted_rule = formatted_rule + str(movie) + ','\n",
        "        association_rules.append(formatted_rule.rstrip(','))"
      ],
      "metadata": {
        "id": "uDD0trjoKX45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=1\n",
        "for rule in association_rules:\n",
        "  print(str(i) + '. ' + rule)\n",
        "  i=i+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuUNy0DjKkl4",
        "outputId": "70a1ed6a-538f-470e-c6a4-3b54fd9f371a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Lord of the Rings: The Fellowship of the Ring, The (2001) => Lord of the Rings: The Return of the King, The (2003)\n",
            "2. Lord of the Rings: The Return of the King, The (2003) => Lord of the Rings: The Fellowship of the Ring, The (2001)\n",
            "3. Jurassic Park (1993) => Forrest Gump (1994)\n",
            "4. Star Wars: Episode V - The Empire Strikes Back (1980) => Star Wars: Episode IV - A New Hope (1977)\n",
            "5. Star Wars: Episode VI - Return of the Jedi (1983) => Star Wars: Episode IV - A New Hope (1977)\n",
            "6. Lord of the Rings: The Fellowship of the Ring, The (2001) => Lord of the Rings: The Two Towers, The (2002)\n",
            "7. Lord of the Rings: The Two Towers, The (2002) => Lord of the Rings: The Fellowship of the Ring, The (2001)\n",
            "8. Star Wars: Episode VI - Return of the Jedi (1983) => Star Wars: Episode V - The Empire Strikes Back (1980)\n",
            "9. Seven (a.k.a. Se7en) (1995) => Pulp Fiction (1994)\n",
            "10. Lord of the Rings: The Return of the King, The (2003) => Lord of the Rings: The Two Towers, The (2002)\n",
            "11. Lord of the Rings: The Two Towers, The (2002) => Lord of the Rings: The Return of the King, The (2003)\n",
            "12. Apollo 13 (1995) => Forrest Gump (1994)\n",
            "13. Silence of the Lambs, The (1991),Shawshank Redemption, The (1994) => Pulp Fiction (1994)\n",
            "14. Pulp Fiction (1994),Silence of the Lambs, The (1991) => Shawshank Redemption, The (1994)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement Random Sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "FfeufQAxNB82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_sampled_freq_itemsets(original_txs_list= None, original_freq_itemsets = None, alpha_list = None, relax_factor_list = None, minsup = 50, change_minsup = True, check_false_positives=True):\n",
        "  '''\n",
        "  inputs:\n",
        "  original_txs_list : unsampled list of transactions, used for sampling\n",
        "  original_freq_itemsets : set of frequent itemsets generated without sampling, used to check for false positives\n",
        "  alpha_list : list of the random sampling factors(alpha) to be tested\n",
        "  relax_factor_list : additional scaling factor for the minsup when random sampling is done\n",
        "  minsup: minsup value to be considered (without sampling)\n",
        "  change_minsup : flag used to enable/disable changign minsup value after random sampling\n",
        "  check_false_positives: flag used to enable/disable checking for false positives\n",
        "\n",
        "  Output:\n",
        "  prints out the frequent itemsets for different lengths\n",
        "  '''\n",
        "\n",
        "  for alpha in alpha_list:\n",
        "    print(\"alpha = {}\".format(alpha))\n",
        "    print(\"------------\")\n",
        "    for relax_factor in relax_factor_list:\n",
        "\n",
        "      sample_size = math.ceil(len(original_txs_list)*alpha)\n",
        "      sampled_txs = [original_txs_list[i] for i in sorted(random.sample(range(len(original_txs_list)), sample_size))]\n",
        "\n",
        "      print(\"size of sampled database: {}\".format(len(sampled_txs)))\n",
        "\n",
        "      initial_candidate_list = set() #set having individual movies as sets\n",
        "\n",
        "      for movie_set in sampled_txs:\n",
        "        for movie in movie_set:\n",
        "          initial_candidate_list.add(frozenset([movie]))\n",
        "\n",
        "      #print(len(initial_candidate_list))\n",
        "      frequencies_of_freq_itemsets = defaultdict(int)\n",
        "\n",
        "      new_minsup = minsup\n",
        "      print(\" \")\n",
        "      print(\"minsup = {}\".format(minsup))\n",
        "\n",
        "      if(change_minsup == True):\n",
        "        new_minsup = minsup * alpha * relax_factor\n",
        "        #print(\"------------\")\n",
        "        print(\"relax_factor = {}\".format(relax_factor))\n",
        "        print(\"minsup_sampled = {}\".format(new_minsup))\n",
        "        #print(\"------------\")\n",
        "        print(\" \")\n",
        "\n",
        "      print(\"Size of candidate itemsets for k = 1: {}\".format(len(initial_candidate_list)))\n",
        "\n",
        "      singleton_freq_itemset = get_freq_itemsets(initial_candidate_list, sampled_txs, new_minsup, frequencies_of_freq_itemsets)\n",
        "\n",
        "      total_freq_itemset_count = len(singleton_freq_itemset)\n",
        "\n",
        "      print(\"Size of filtered itemsets for k = 1: {}\".format(len(singleton_freq_itemset)))\n",
        "      print(\" \")\n",
        "\n",
        "      final_itemsets_sampled = list() #list of sets where each set contains itemsets indexed by the length of itemsets\n",
        "      final_itemsets_sampled.append(set())\n",
        "      final_itemsets_sampled.append(singleton_freq_itemset)\n",
        "\n",
        "      k = 2\n",
        "\n",
        "      prev = singleton_freq_itemset\n",
        "\n",
        "      while(len(prev) > 1):\n",
        "        #create new extended itemsets\n",
        "        candidates = get_candidate_itemsets(prev, k)\n",
        "        prev = candidates\n",
        "        print(\"Size of candidate itemsets for k = {}: {}\".format(k, len(candidates)))\n",
        "        if(len(candidates) > 0):\n",
        "          result = get_freq_itemsets(candidates, sampled_txs, new_minsup, frequencies_of_freq_itemsets)\n",
        "          if(len(result)>0):\n",
        "            final_itemsets_sampled.append(result)\n",
        "          prev = result\n",
        "          print(\"Size of filtered itemsets for k = {}: {}\".format(k, len(result)))\n",
        "          total_freq_itemset_count+=len(result)\n",
        "        k=k+1\n",
        "        print(\" \")\n",
        "\n",
        "      print(\"Total number of frequent itemsets found = {}\".format(total_freq_itemset_count))\n",
        "\n",
        "      if(check_false_positives == True):\n",
        "        j=1\n",
        "        false_positives = np.zeros(len(final_itemsets_sampled))\n",
        "        for itemsets in final_itemsets_sampled[1:]:\n",
        "          for itemset in itemsets:\n",
        "            if(j < len(original_freq_itemsets)):#if original freq itemset list does not contain itemsets of this length, then mark all the freq itemsets as false positives\n",
        "              if(itemset not in original_freq_itemsets[j]):\n",
        "                false_positives[j]+=1\n",
        "            else:\n",
        "              false_positives[j] = len(final_itemsets_sampled[j])\n",
        "          print(\"False positive count in itemsets of length k:{} = {}\".format(j,int(false_positives[j])))\n",
        "          j=j+1\n",
        "        print(\"Total number of false positives found = {}\".format(int(sum(false_positives))))\n",
        "\n",
        "      print(\" \")\n",
        "\n"
      ],
      "metadata": {
        "id": "bslz87rc9kET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_list = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9,1]\n",
        "relax_factor_list = [1]\n",
        "original_txs_list = txs_list\n",
        "original_freq_itemsets = final_itemsets\n",
        "minsup = 150\n",
        "get_random_sampled_freq_itemsets(original_txs_list, original_freq_itemsets, alpha_list, relax_factor_list, minsup,False, False )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQZ_gl2HCRR5",
        "outputId": "05a1c4b2-b709-4e8e-aea2-eb33200b1471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha = 0.4\n",
            "------------\n",
            "size of sampled database: 244\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 6421\n",
            "Size of filtered itemsets for k = 1: 0\n",
            " \n",
            "Total number of frequent itemsets found = 0\n",
            " \n",
            "alpha = 0.5\n",
            "------------\n",
            "size of sampled database: 305\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 6240\n",
            "Size of filtered itemsets for k = 1: 2\n",
            " \n",
            "Size of candidate itemsets for k = 2: 1\n",
            "Size of filtered itemsets for k = 2: 0\n",
            " \n",
            "Total number of frequent itemsets found = 2\n",
            " \n",
            "alpha = 0.6\n",
            "------------\n",
            "size of sampled database: 366\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 5486\n",
            "Size of filtered itemsets for k = 1: 4\n",
            " \n",
            "Size of candidate itemsets for k = 2: 6\n",
            "Size of filtered itemsets for k = 2: 0\n",
            " \n",
            "Total number of frequent itemsets found = 4\n",
            " \n",
            "alpha = 0.7\n",
            "------------\n",
            "size of sampled database: 427\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 7448\n",
            "Size of filtered itemsets for k = 1: 8\n",
            " \n",
            "Size of candidate itemsets for k = 2: 28\n",
            "Size of filtered itemsets for k = 2: 0\n",
            " \n",
            "Total number of frequent itemsets found = 8\n",
            " \n",
            "alpha = 0.8\n",
            "------------\n",
            "size of sampled database: 488\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 7957\n",
            "Size of filtered itemsets for k = 1: 14\n",
            " \n",
            "Size of candidate itemsets for k = 2: 91\n",
            "Size of filtered itemsets for k = 2: 3\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1\n",
            "Size of filtered itemsets for k = 3: 0\n",
            " \n",
            "Total number of frequent itemsets found = 17\n",
            " \n",
            "alpha = 0.9\n",
            "------------\n",
            "size of sampled database: 549\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 8232\n",
            "Size of filtered itemsets for k = 1: 30\n",
            " \n",
            "Size of candidate itemsets for k = 2: 435\n",
            "Size of filtered itemsets for k = 2: 10\n",
            " \n",
            "Size of candidate itemsets for k = 3: 13\n",
            "Size of filtered itemsets for k = 3: 0\n",
            " \n",
            "Total number of frequent itemsets found = 40\n",
            " \n",
            "alpha = 1\n",
            "------------\n",
            "size of sampled database: 609\n",
            " \n",
            "minsup = 150\n",
            "Size of candidate itemsets for k = 1: 8452\n",
            "Size of filtered itemsets for k = 1: 37\n",
            " \n",
            "Size of candidate itemsets for k = 2: 666\n",
            "Size of filtered itemsets for k = 2: 30\n",
            " \n",
            "Size of candidate itemsets for k = 3: 87\n",
            "Size of filtered itemsets for k = 3: 2\n",
            " \n",
            "Size of candidate itemsets for k = 4: 1\n",
            "Size of filtered itemsets for k = 4: 0\n",
            " \n",
            "Total number of frequent itemsets found = 69\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "1.   alpha is varied while keeping the minsup constant.\n",
        "2.   Reducing the dataset size alone (no change in minsup) by random sampling certainly reduces the number of frequent itemsets discovered.\n",
        "3.   As seen in the above results, the number of frequent itemsets increases with the increase in alpha. This is because more transactions are considered with higher value of alpha which increases the likelihood of finding a frequent itemset.\n",
        "\n"
      ],
      "metadata": {
        "id": "rIBMkM-W-8Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Check for False Positives\n",
        "\n",
        "Next let's verify that the candidate pairs we discover by random sampling are truly frequent by comparing to the itemsets we discover over the entire dataset.\n",
        "\n",
        "For this part, consider another parameter **minsup_sample** that relaxes the minimum support threshold. For example if we want minsup = 1/100 for whole dataset, then try minsup_sample = 1/125 for the sample. This will help catch truly frequent itemsets.\n"
      ],
      "metadata": {
        "id": "wLNDiFDrI2-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "relax_factor_list = [0.8]\n",
        "\n",
        "original_txs_list = txs_list\n",
        "original_freq_itemsets = final_itemsets\n",
        "\n",
        "minsup_orignal = 150\n",
        "get_random_sampled_freq_itemsets(original_txs_list, original_freq_itemsets, alpha_list, relax_factor_list, minsup_orignal, True, True )"
      ],
      "metadata": {
        "id": "R8gtIPuf_82B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05403ee0-c09a-4090-b3f7-ec5c582e0776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha = 0.1\n",
            "------------\n",
            "size of sampled database: 61\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 12.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 3502\n",
            "Size of filtered itemsets for k = 1: 79\n",
            " \n",
            "Size of candidate itemsets for k = 2: 3081\n",
            "Size of filtered itemsets for k = 2: 157\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1470\n",
            "Size of filtered itemsets for k = 3: 57\n",
            " \n",
            "Size of candidate itemsets for k = 4: 178\n",
            "Size of filtered itemsets for k = 4: 8\n",
            " \n",
            "Size of candidate itemsets for k = 5: 7\n",
            "Size of filtered itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 301\n",
            "False positive count in itemsets of length k:1 = 42\n",
            "False positive count in itemsets of length k:2 = 131\n",
            "False positive count in itemsets of length k:3 = 55\n",
            "False positive count in itemsets of length k:4 = 8\n",
            "Total number of false positives found = 236\n",
            " \n",
            "alpha = 0.2\n",
            "------------\n",
            "size of sampled database: 122\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 24.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 4985\n",
            "Size of filtered itemsets for k = 1: 66\n",
            " \n",
            "Size of candidate itemsets for k = 2: 2145\n",
            "Size of filtered itemsets for k = 2: 103\n",
            " \n",
            "Size of candidate itemsets for k = 3: 763\n",
            "Size of filtered itemsets for k = 3: 28\n",
            " \n",
            "Size of candidate itemsets for k = 4: 42\n",
            "Size of filtered itemsets for k = 4: 2\n",
            " \n",
            "Size of candidate itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 199\n",
            "False positive count in itemsets of length k:1 = 29\n",
            "False positive count in itemsets of length k:2 = 74\n",
            "False positive count in itemsets of length k:3 = 26\n",
            "False positive count in itemsets of length k:4 = 2\n",
            "Total number of false positives found = 131\n",
            " \n",
            "alpha = 0.3\n",
            "------------\n",
            "size of sampled database: 183\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 36.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 5899\n",
            "Size of filtered itemsets for k = 1: 98\n",
            " \n",
            "Size of candidate itemsets for k = 2: 4753\n",
            "Size of filtered itemsets for k = 2: 338\n",
            " \n",
            "Size of candidate itemsets for k = 3: 4905\n",
            "Size of filtered itemsets for k = 3: 250\n",
            " \n",
            "Size of candidate itemsets for k = 4: 1457\n",
            "Size of filtered itemsets for k = 4: 55\n",
            " \n",
            "Size of candidate itemsets for k = 5: 130\n",
            "Size of filtered itemsets for k = 5: 2\n",
            " \n",
            "Size of candidate itemsets for k = 6: 1\n",
            "Size of filtered itemsets for k = 6: 0\n",
            " \n",
            "Total number of frequent itemsets found = 743\n",
            "False positive count in itemsets of length k:1 = 61\n",
            "False positive count in itemsets of length k:2 = 308\n",
            "False positive count in itemsets of length k:3 = 248\n",
            "False positive count in itemsets of length k:4 = 55\n",
            "False positive count in itemsets of length k:5 = 2\n",
            "Total number of false positives found = 674\n",
            " \n",
            "alpha = 0.4\n",
            "------------\n",
            "size of sampled database: 244\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 48.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 5723\n",
            "Size of filtered itemsets for k = 1: 68\n",
            " \n",
            "Size of candidate itemsets for k = 2: 2278\n",
            "Size of filtered itemsets for k = 2: 126\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1125\n",
            "Size of filtered itemsets for k = 3: 35\n",
            " \n",
            "Size of candidate itemsets for k = 4: 74\n",
            "Size of filtered itemsets for k = 4: 5\n",
            " \n",
            "Size of candidate itemsets for k = 5: 2\n",
            "Size of filtered itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 234\n",
            "False positive count in itemsets of length k:1 = 31\n",
            "False positive count in itemsets of length k:2 = 96\n",
            "False positive count in itemsets of length k:3 = 33\n",
            "False positive count in itemsets of length k:4 = 5\n",
            "Total number of false positives found = 165\n",
            " \n",
            "alpha = 0.5\n",
            "------------\n",
            "size of sampled database: 305\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 60.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 6545\n",
            "Size of filtered itemsets for k = 1: 64\n",
            " \n",
            "Size of candidate itemsets for k = 2: 2016\n",
            "Size of filtered itemsets for k = 2: 118\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1037\n",
            "Size of filtered itemsets for k = 3: 39\n",
            " \n",
            "Size of candidate itemsets for k = 4: 73\n",
            "Size of filtered itemsets for k = 4: 4\n",
            " \n",
            "Size of candidate itemsets for k = 5: 1\n",
            "Size of filtered itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 225\n",
            "False positive count in itemsets of length k:1 = 27\n",
            "False positive count in itemsets of length k:2 = 88\n",
            "False positive count in itemsets of length k:3 = 37\n",
            "False positive count in itemsets of length k:4 = 4\n",
            "Total number of false positives found = 156\n",
            " \n",
            "alpha = 0.6\n",
            "------------\n",
            "size of sampled database: 366\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 72.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 7197\n",
            "Size of filtered itemsets for k = 1: 67\n",
            " \n",
            "Size of candidate itemsets for k = 2: 2211\n",
            "Size of filtered itemsets for k = 2: 145\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1467\n",
            "Size of filtered itemsets for k = 3: 54\n",
            " \n",
            "Size of candidate itemsets for k = 4: 134\n",
            "Size of filtered itemsets for k = 4: 2\n",
            " \n",
            "Size of candidate itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 268\n",
            "False positive count in itemsets of length k:1 = 30\n",
            "False positive count in itemsets of length k:2 = 115\n",
            "False positive count in itemsets of length k:3 = 52\n",
            "False positive count in itemsets of length k:4 = 2\n",
            "Total number of false positives found = 199\n",
            " \n",
            "alpha = 0.7\n",
            "------------\n",
            "size of sampled database: 427\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 84.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 7350\n",
            "Size of filtered itemsets for k = 1: 67\n",
            " \n",
            "Size of candidate itemsets for k = 2: 2211\n",
            "Size of filtered itemsets for k = 2: 131\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1297\n",
            "Size of filtered itemsets for k = 3: 46\n",
            " \n",
            "Size of candidate itemsets for k = 4: 93\n",
            "Size of filtered itemsets for k = 4: 2\n",
            " \n",
            "Size of candidate itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 246\n",
            "False positive count in itemsets of length k:1 = 30\n",
            "False positive count in itemsets of length k:2 = 101\n",
            "False positive count in itemsets of length k:3 = 44\n",
            "False positive count in itemsets of length k:4 = 2\n",
            "Total number of false positives found = 177\n",
            " \n",
            "alpha = 0.8\n",
            "------------\n",
            "size of sampled database: 488\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 96.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 7837\n",
            "Size of filtered itemsets for k = 1: 60\n",
            " \n",
            "Size of candidate itemsets for k = 2: 1770\n",
            "Size of filtered itemsets for k = 2: 109\n",
            " \n",
            "Size of candidate itemsets for k = 3: 901\n",
            "Size of filtered itemsets for k = 3: 28\n",
            " \n",
            "Size of candidate itemsets for k = 4: 51\n",
            "Size of filtered itemsets for k = 4: 0\n",
            " \n",
            "Total number of frequent itemsets found = 197\n",
            "False positive count in itemsets of length k:1 = 23\n",
            "False positive count in itemsets of length k:2 = 79\n",
            "False positive count in itemsets of length k:3 = 26\n",
            "Total number of false positives found = 128\n",
            " \n",
            "alpha = 0.9\n",
            "------------\n",
            "size of sampled database: 549\n",
            " \n",
            "minsup = 150\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 108.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 8290\n",
            "Size of filtered itemsets for k = 1: 68\n",
            " \n",
            "Size of candidate itemsets for k = 2: 2278\n",
            "Size of filtered itemsets for k = 2: 122\n",
            " \n",
            "Size of candidate itemsets for k = 3: 1056\n",
            "Size of filtered itemsets for k = 3: 38\n",
            " \n",
            "Size of candidate itemsets for k = 4: 73\n",
            "Size of filtered itemsets for k = 4: 1\n",
            " \n",
            "Total number of frequent itemsets found = 229\n",
            "False positive count in itemsets of length k:1 = 31\n",
            "False positive count in itemsets of length k:2 = 92\n",
            "False positive count in itemsets of length k:3 = 36\n",
            "False positive count in itemsets of length k:4 = 1\n",
            "Total number of false positives found = 160\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "*   In the above code, minsup is scaled proportionately in accordance with the random sampling factor (alpha)\n",
        "*   The minsup is additionally scaled by a relaxation factor, set to 0.8\n",
        "*   Reducing the dataset size alone (no change in minsup) by random sampling certainly reduces the number of frequent itemsets discovered as seen in the previous part.\n",
        "*   Scaling the minsup along with random sampling gives more frequent itemsets.\n",
        "*   Many of the frequent itemsets found this way have low frequency in the original dataset and hence do not qualify as frequent itemsets. These are marked as false positives.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tgpWjmXh_91h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Extensions and Next Steps\n",
        "\n",
        "So far, we have been working with a fairly small dataset. For this last question, try your sampling-based approach on the much larger: **Movies 10M** dataset: https://files.grouplens.org/datasets/movielens/ml-10m.zip\n",
        "\n",
        "First, we need to load this larger dataset:"
      ],
      "metadata": {
        "id": "Wt4JCrwcAHal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "import zipfile\n",
        "\n",
        "http = urllib3.PoolManager()\n",
        "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-10m.zip\", preload_content=False)\n",
        "\n",
        "with open(\"movie.zip\", 'wb') as out:\n",
        "  while True:\n",
        "    data = req.read(4096)\n",
        "    if not data:\n",
        "      break\n",
        "    out.write(data)\n",
        "req.release_conn()\n",
        "\n",
        "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
        "for fileM in zFile.namelist():\n",
        "  zFile.extract(fileM)"
      ],
      "metadata": {
        "id": "iG7qBkj7AVou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a07dbdc-e335-4812-f7a7-3563e1e4625e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ml-10M100K/"
      ],
      "metadata": {
        "id": "6Hi45CqJht7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b26d5f8-44b5-4c74-d409-7caa82a0541b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allbut.pl  movies.dat  ratings.dat  README.html  split_ratings.sh  tags.dat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# read user ratings\n",
        "allRatings = pd.read_csv(\"ml-10M100K/ratings.dat\",sep='::', names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"], engine='python')\n",
        "allRatings"
      ],
      "metadata": {
        "id": "V_jdR72WiR2F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "45ed5de0-8c59-4711-904f-349c7e0dfbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e03f25f5-ef07-4711-a241-c8b1dca24abe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>122</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838985046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>185</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>231</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>292</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>316</td>\n",
              "      <td>5.0</td>\n",
              "      <td>838983392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000049</th>\n",
              "      <td>71567</td>\n",
              "      <td>2107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>912580553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000050</th>\n",
              "      <td>71567</td>\n",
              "      <td>2126</td>\n",
              "      <td>2.0</td>\n",
              "      <td>912649143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000051</th>\n",
              "      <td>71567</td>\n",
              "      <td>2294</td>\n",
              "      <td>5.0</td>\n",
              "      <td>912577968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000052</th>\n",
              "      <td>71567</td>\n",
              "      <td>2338</td>\n",
              "      <td>2.0</td>\n",
              "      <td>912578016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000053</th>\n",
              "      <td>71567</td>\n",
              "      <td>2384</td>\n",
              "      <td>2.0</td>\n",
              "      <td>912578173</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000054 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e03f25f5-ef07-4711-a241-c8b1dca24abe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e03f25f5-ef07-4711-a241-c8b1dca24abe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e03f25f5-ef07-4711-a241-c8b1dca24abe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          userId  movieId  rating  timestamp\n",
              "0              1      122     5.0  838985046\n",
              "1              1      185     5.0  838983525\n",
              "2              1      231     5.0  838983392\n",
              "3              1      292     5.0  838983421\n",
              "4              1      316     5.0  838983392\n",
              "...          ...      ...     ...        ...\n",
              "10000049   71567     2107     1.0  912580553\n",
              "10000050   71567     2126     2.0  912649143\n",
              "10000051   71567     2294     5.0  912577968\n",
              "10000052   71567     2338     2.0  912578016\n",
              "10000053   71567     2384     2.0  912578173\n",
              "\n",
              "[10000054 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can begin your sampling over this larger dataset."
      ],
      "metadata": {
        "id": "WEX9wu7ewIqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "from collections import defaultdict\n",
        "\n",
        "#keep reviews with ratings >=3 only\n",
        "ratings_new = allRatings[allRatings['rating']>=3]\n",
        "\n",
        "#initial itemset is the set of singleton movie ids\n",
        "ratings_new = ratings_new.groupby('userId')\n",
        "initial_candidate_list = set() #set having individual movies as sets\n",
        "txs_list = [] #database of transactions\n",
        "for userId, data in ratings_new:\n",
        "  txs_list.append(frozenset(data['movieId']))\n",
        "  for movie in data['movieId']:\n",
        "    initial_candidate_list.add(frozenset([movie]))"
      ],
      "metadata": {
        "id": "VYRlIEyulq2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(txs_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HhNMOouA_qP",
        "outputId": "17aa947c-c04e-479e-fde3-084b36203e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_list = [0.1]\n",
        "relax_factor_list = [0.8]\n",
        "original_txs_list = txs_list\n",
        "original_freq_itemsets = None\n",
        "\n",
        "minsup = 15000  # in the small movies data set, #transactions was around 600 and minsup was taken as 150. So choosing 15000 as minsup in the same proportion\n",
        "get_random_sampled_freq_itemsets(original_txs_list, original_freq_itemsets, alpha_list, relax_factor_list, minsup,True, False )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oryVlMBD5CPq",
        "outputId": "c57addcb-07d8-48e8-bdee-e5145f444e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha = 0.1\n",
            "------------\n",
            "size of sampled database: 6987\n",
            " \n",
            "minsup = 15000\n",
            "relax_factor = 0.8\n",
            "minsup_sampled = 1200.0\n",
            " \n",
            "Size of candidate itemsets for k = 1: 9583\n",
            "Size of filtered itemsets for k = 1: 94\n",
            " \n",
            "Size of candidate itemsets for k = 2: 4371\n",
            "Size of filtered itemsets for k = 2: 293\n",
            " \n",
            "Size of candidate itemsets for k = 3: 3324\n",
            "Size of filtered itemsets for k = 3: 223\n",
            " \n",
            "Size of candidate itemsets for k = 4: 871\n",
            "Size of filtered itemsets for k = 4: 28\n",
            " \n",
            "Size of candidate itemsets for k = 5: 60\n",
            "Size of filtered itemsets for k = 5: 0\n",
            " \n",
            "Total number of frequent itemsets found = 638\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   I have set the minsup to 15000 (proportionate to the previous case).\n",
        "*   10% of the total samples are considered for analysis. Further, the minsup is scaled by 0.1 to account for the sampling and further relaxed by scaling by 0.8. This effectively makes the minsup as 1200.\n",
        "*   The total number of frequent itemsets discovered are 638.\n",
        "\n"
      ],
      "metadata": {
        "id": "FX3LDkfAlpyg"
      }
    }
  ]
}